{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOJp_QfYajVk"
      },
      "source": [
        "## Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L13CC9--ZbmA",
        "outputId": "fef8e847-6b58-4dac-aa71-b1f9c60fbca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.1.4)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.0.4)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis) (3.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (3.0.45)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9.6->pyvis) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.13)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in _compute_dependencies\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in <listcomp>\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3124, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 300, in evaluate\n",
            "    current_environment = default_environment()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers pyvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huw4xXHvZj9b"
      },
      "outputs": [],
      "source": [
        "#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "from typing_extensions import DefaultDict\n",
        "#from pyvis.network import Network\n",
        "import IPython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go1sK_O6lx2B"
      },
      "source": [
        "##Read From CSV\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "nuoxLDermF0h",
        "outputId": "e3930e42-2535-40c0-a035-3af7d8d2d7df"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"harry_potter_books.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70ZRKgvPnlnA"
      },
      "outputs": [],
      "source": [
        "books = set(df['book'])\n",
        "\n",
        "def extract_text(title):\n",
        "  contents = ''\n",
        "  temp_book = df[df['book'] == title]['text']\n",
        "  for i in temp_book:\n",
        "    contents += i\n",
        "  return contents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kabfWxCTZnTf"
      },
      "source": [
        "## Load the Relation Extraction Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "789e1ada961b4a4784a28a3fb8c46e03",
            "f466a1e7bc6344aa9e1c3712e932ceb2",
            "7c7e1d93f6ee4915a4212550d75bb265",
            "d246dc287bf841c7b1d42a173f9804f5",
            "17a0b4224cbd4d8c9ed0c9af28b79a57",
            "4ce44de7732946ac82318017ef17346d",
            "22100dde11174116ab2630923616c6a9",
            "5c90adfa129942a0812620e721321230"
          ]
        },
        "id": "w-E86e2dZlmz",
        "outputId": "b84e7972-061f-4d06-82a3-8241bd8fb560"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekGnutl3ZzTz"
      },
      "source": [
        "## From Long Text to KB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ99_R18Z1_M"
      },
      "outputs": [],
      "source": [
        "class KB():\n",
        "    def __init__(self):\n",
        "        self.relations = []\n",
        "\n",
        "    def text_normalize(self,text):\n",
        "        text = text.lower()\n",
        "        text = text.replace(\"'s\",\"\")\n",
        "        return text\n",
        "\n",
        "    def are_relations_equal(self, r1, r2):\n",
        "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
        "\n",
        "    def exists_relation(self, r1):\n",
        "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
        "\n",
        "    def merge_relations(self, r1):\n",
        "        for r2 in self.relations:\n",
        "            if self.are_relations_equal(r1, r2):\n",
        "                # Merge spans\n",
        "                spans_to_add = [span for span in r1[\"meta\"][\"spans\"] if span not in r2[\"meta\"][\"spans\"]]\n",
        "                r2[\"meta\"][\"spans\"] += spans_to_add\n",
        "                # Increment the count of occurrences\n",
        "                r2[\"count\"] += 1\n",
        "                break\n",
        "\n",
        "    def add_relation(self, r):\n",
        "        r['head'] = self.text_normalize(r['head'])\n",
        "        r['tail'] = self.text_normalize(r['tail'])\n",
        "        r['count'] = 1\n",
        "\n",
        "        if not self.exists_relation(r):\n",
        "            self.relations.append(r)\n",
        "        else:\n",
        "            self.merge_relations(r)\n",
        "\n",
        "    def merge_kb(self, other_kb):\n",
        "        for relation in other_kb.relations:\n",
        "            self.add_relation(relation)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"Relations:\")\n",
        "        for r in self.relations:\n",
        "            print(f\"  {r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AmnT5dAnOs8"
      },
      "outputs": [],
      "source": [
        "def extract_relations_from_model_output(text):\n",
        "    relations = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
        "    for token in text_replaced.split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        relations.append({\n",
        "            'head': subject.strip(),\n",
        "            'type': relation.strip(),\n",
        "            'tail': object_.strip()\n",
        "        })\n",
        "    return relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ERtBsrPZ4BF"
      },
      "outputs": [],
      "source": [
        "# extract relations for each span and put them together in a knowledge base\n",
        "def from_text_to_kb(text, span_length=128, verbose=False):\n",
        "    # tokenize whole text\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "    # compute span boundaries\n",
        "    num_tokens = len(inputs[\"input_ids\"][0])\n",
        "    if verbose:\n",
        "        print(f\"Input has {num_tokens} tokens\")\n",
        "    num_spans = math.ceil(num_tokens / span_length)\n",
        "    if verbose:\n",
        "        print(f\"Input has {num_spans} spans\")\n",
        "    overlap = math.ceil((num_spans * span_length - num_tokens) /\n",
        "                        max(num_spans - 1, 1))\n",
        "    spans_boundaries = []\n",
        "    start = 0\n",
        "    for i in range(num_spans):\n",
        "        spans_boundaries.append([start + span_length * i,\n",
        "                                 start + span_length * (i + 1)])\n",
        "        start -= overlap\n",
        "    if verbose:\n",
        "        print(f\"Span boundaries are {spans_boundaries}\")\n",
        "\n",
        "    # transform input with spans\n",
        "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
        "                  for boundary in spans_boundaries]\n",
        "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
        "                    for boundary in spans_boundaries]\n",
        "    inputs = {\n",
        "        \"input_ids\": torch.stack(tensor_ids),\n",
        "        \"attention_mask\": torch.stack(tensor_masks)\n",
        "    }\n",
        "\n",
        "    # generate relations\n",
        "    num_return_sequences = 3\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": 256,\n",
        "        \"length_penalty\": 0,\n",
        "        \"num_beams\": 3,\n",
        "        \"num_return_sequences\": num_return_sequences\n",
        "    }\n",
        "    generated_tokens = model.generate(\n",
        "        **inputs,\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "\n",
        "    # decode relations\n",
        "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
        "                                           skip_special_tokens=False)\n",
        "\n",
        "    # create kb\n",
        "    kb = KB()\n",
        "    i = 0\n",
        "    for sentence_pred in decoded_preds:\n",
        "        current_span_index = i // num_return_sequences\n",
        "        relations = extract_relations_from_model_output(sentence_pred)\n",
        "        for relation in relations:\n",
        "            relation[\"meta\"] = {\n",
        "                \"spans\": [spans_boundaries[current_span_index]]\n",
        "            }\n",
        "            kb.add_relation(relation)\n",
        "        i += 1\n",
        "\n",
        "    return kb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR70JHFDKpJL",
        "outputId": "cba9a27e-e005-4c67-a733-d89565543ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Relations:\n",
            "  {'head': 'riddle house', 'type': 'located in the administrative territorial entity', 'tail': 'little hangleron', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'little hangleron', 'type': 'instance of', 'tail': 'village', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'the riddle house', 'type': 'located in the administrative territorial entity', 'tail': 'little hangleron', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'riddlehouse', 'type': 'present in work', 'tail': 'the little hagletons', 'meta': {'spans': [[99, 227]]}, 'count': 1}\n",
            "  {'head': 'riddlehouse', 'type': 'occupant', 'tail': 'little hagletons', 'meta': {'spans': [[99, 227]]}, 'count': 1}\n",
            "  {'head': 'riddlehouse', 'type': 'located in the administrative territorial entity', 'tail': 'little haglet', 'meta': {'spans': [[99, 227]]}, 'count': 1}\n",
            "Relations:\n",
            "  {'head': 'riddle house', 'type': 'located in the administrative territorial entity', 'tail': 'little hangleron', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'little hangleron', 'type': 'instance of', 'tail': 'village', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'the riddle house', 'type': 'located in the administrative territorial entity', 'tail': 'little hangleron', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'riddlehouse', 'type': 'present in work', 'tail': 'the little hagletons', 'meta': {'spans': [[99, 227]]}, 'count': 1}\n",
            "  {'head': 'riddlehouse', 'type': 'occupant', 'tail': 'little hagletons', 'meta': {'spans': [[99, 227]]}, 'count': 1}\n",
            "  {'head': 'riddlehouse', 'type': 'located in the administrative territorial entity', 'tail': 'little haglet', 'meta': {'spans': [[99, 227]]}, 'count': 1}\n",
            "  {'head': 'little hangleton', 'type': 'instance of', 'tail': 'village', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'little hangleton', 'type': 'located in the administrative territorial entity', 'tail': 'the village', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'the riddles', 'type': 'narrative location', 'tail': 'little hangleton', 'meta': {'spans': [[0, 128]]}, 'count': 1}\n",
            "  {'head': 'the hanged man', 'type': 'instance of', 'tail': 'pub', 'meta': {'spans': [[107, 235]]}, 'count': 1}\n",
            "  {'head': 'mr. and mrs. riddle', 'type': 'child', 'tail': 'tom', 'meta': {'spans': [[107, 235]]}, 'count': 1}\n",
            "  {'head': 'mr.and mrs. riddle', 'type': 'child', 'tail': 'tom', 'meta': {'spans': [[107, 235]]}, 'count': 1}\n",
            "Book 4: Goblet of Fire\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for ch in books:\n",
        "  text = extract_text(ch)\n",
        "  kb = None\n",
        "  for i in range(0,len(text)-1000,1000):\n",
        "    if not kb:\n",
        "      kb = from_text_to_kb(text[i:i+1000])\n",
        "      kb.print()\n",
        "    else:\n",
        "      new = from_text_to_kb(text[i:i+1000])\n",
        "      kb.merge_kb(new)\n",
        "      kb.print()\n",
        "\n",
        "  json_data = json.dumps(kb.relations, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08RukxO23PsX"
      },
      "source": [
        "## Entity Normalization and Remove Redundant Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTfUXZrV7cxC"
      },
      "outputs": [],
      "source": [
        "def read_relations(file_path):\n",
        "  with open(file_path, 'r') as file:\n",
        "    relations = json.load(file)\n",
        "  return relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ28kXi73ZA2"
      },
      "outputs": [],
      "source": [
        "def entity_normalization(relations):\n",
        "  def extract_synonyms(relations,synonyms):\n",
        "      if relations['type'] in ['subclass of','instance of']:\n",
        "          synonyms[relations['tail']].append(relations['head'])\n",
        "\n",
        "  def build_entity_syn(relations,syn):\n",
        "    synonyms = DefaultDict(list)\n",
        "    for row in relations:\n",
        "      extract_synonyms(row,synonyms)\n",
        "    reverse_synonyms = {}\n",
        "    for key, values in synonyms.items():\n",
        "        for value in values:\n",
        "            reverse_synonyms[value] = key\n",
        "    return reverse_synonyms\n",
        "\n",
        "  def replace_entity_and_build_syn_connection(relations,syn):\n",
        "    for rel in relations:\n",
        "      head = rel['head']\n",
        "      if head in syn:\n",
        "        rel['head'] = syn[head]\n",
        "\n",
        "    for key,value in syn.items():\n",
        "      relations.append({\n",
        "          'head': key,\n",
        "          'type': 'subclass of',\n",
        "          'tail': value,\n",
        "          'meta':{\n",
        "              'type':'synonyms'\n",
        "          },\n",
        "          'count': 1\n",
        "      })\n",
        "    return relations\n",
        "  syn = build_entity_syn(relations)\n",
        "  file = replace_entity_and_build_syn_connection(relations,syn)\n",
        "  return file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKNcx7nNb8m3"
      },
      "outputs": [],
      "source": [
        "def preprocess_json(file_name):\n",
        "  file = read_relations(file_name)\n",
        "  file = entity_normalization(file)\n",
        "  return file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r6Ko6xjBwT0"
      },
      "source": [
        "\n",
        "## hand craft to normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMbeFX8oB0Xv"
      },
      "outputs": [],
      "source": [
        "file = read_relations(\"ch1-ch6 merged.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koflq1rJDGuj"
      },
      "outputs": [],
      "source": [
        "entity_synonyms = {\n",
        "    \"harry potter\": [\"the boy who lived\", \"the chosen one\", \"'arry potter\", \"harry\", \"potter\",\"'arry potter\"],\n",
        "    \"hermione granger\": [\"hermione\", \"miss granger\", \"granger\"],\n",
        "    \"ron weasley\": [\"ron\", \"ronald weasley\", \"weasley\"],\n",
        "    \"albus dumbledore\": [\"dumbledore\", \"professor dumbledore\", \"albus\"],\n",
        "    \"severus snape\": [\"snape\", \"professor snape\", \"severus\"],\n",
        "    \"voldemort\": [\"he-who-must-not-be-named\", \"you-know-who\", \"the dark lord\", \"tom riddle\", \"lord voldemort\"],\n",
        "    \"hagrid\": [\"rubeus hagrid\", \"'agrid\", \"gamekeeper\"],\n",
        "    \"draco malfoy\": [\"draco\", \"malfoy\"],\n",
        "    \"sirius black\": [\"sirius\", \"padfoot\", \"black\"],\n",
        "    \"remus lupin\": [\"lupin\", \"moony\", \"professor lupin\"],\n",
        "    \"peter pettigrew\": [\"wormtail\", \"pettigrew\"],\n",
        "    \"ginny weasley\": [\"ginny\", \"ginevra weasley\"],\n",
        "    \"neville longbottom\": [\"neville\", \"longbottom\"],\n",
        "    \"luna lovegood\": [\"luna\", \"'loony' lovegood\"],\n",
        "    \"dobby\": [\"the house elf\", \"dobby\", \"free elf\"],\n",
        "    \"hogwarts\": [\"hogwarts school of witchcraft and wizardry\", \"school of witchcraft and wizardry\"],\n",
        "    \"quidditch\": [\"the sport of wizards\", \"wizarding sport\"],\n",
        "    \"azkaban\": [\"the wizard prison\", \"wizard prison\"],\n",
        "    \"minerva mcgonagall\": [\"mcgonagall\", \"professor mcgonagall\"],\n",
        "    \"mad-eye moody\": [\"mad-eye\", \"alastor moody\", \"moody\"],\n",
        "    \"bellatrix lestrange\": [\"bellatrix\", \"lestrange\"],\n",
        "    \"dolores umbridge\": [\"umbridge\", \"professor umbridge\"],\n",
        "    \"lucius malfoy\": [\"lucius\", \"mr. malfoy\"],\n",
        "    \"molly weasley\": [\"molly\", \"mrs. weasley\"],\n",
        "    \"arthur weasley\": [\"arthur\", \"mr. weasley\"],\n",
        "    \"fred and george weasley\": [\"fred weasley\", \"george weasley\", \"the twins\", \"weasley twins\"],\n",
        "    \"cedric diggory\": [\"cedric\", \"diggory\"],\n",
        "    \"fleur delacour\": [\"fleur\", \"madame maxime\"],\n",
        "    \"dumbledore's army\": [\"da\", \"the army\"],\n",
        "    \"order of the phoenix\": [\"the order\"],\n",
        "    \"the death eaters\": [\"death eaters\"],\n",
        "    \"the triwizard tournament\": [\"triwizard tournament\"],\n",
        "    \"the prophecy\": [\"prophecy\"],\n",
        "    \"the elder wand\": [\"elder wand\"],\n",
        "    \"the invisibility cloak\": [\"invisibility cloak\"],\n",
        "    \"the resurrection stone\": [\"resurrection stone\"],\n",
        "    \"the marauder's map\": [\"marauder's map\", \"map\"]\n",
        "}\n",
        "reverse_synonyms = {}\n",
        "for key, values in entity_synonyms.items():\n",
        "    for value in values:\n",
        "        reverse_synonyms[value] = key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b0v7lyxCVS8"
      },
      "outputs": [],
      "source": [
        "def process_and_aggregate(file, reverse_synonyms):\n",
        "    processed = {}\n",
        "    for row in file:\n",
        "        # Normalize head and tail by stripping spaces and converting to lower case\n",
        "        head = reverse_synonyms.get(row['head'], row['head'])\n",
        "        tail = reverse_synonyms.get(row['tail'], row['tail'])\n",
        "        if len(head) > 20 or len(tail) > 20:\n",
        "            continue\n",
        "        if '*' in head or '-' in head or '*' in tail or '=' in tail or ',' in head or ',' in tail:\n",
        "            continue\n",
        "        # Continue if head and tail are the same\n",
        "        if head == tail or head.strip() == tail.strip():\n",
        "            continue\n",
        "\n",
        "        rel = row['type']\n",
        "\n",
        "        key = (head, rel, tail)\n",
        "\n",
        "        if key in processed:\n",
        "            processed[key]['count'] += row['count']\n",
        "        else:\n",
        "            processed[key] = {\n",
        "                'head': head,\n",
        "                'type': rel,\n",
        "                'tail': tail,\n",
        "                'count': row['count']\n",
        "            }\n",
        "\n",
        "\n",
        "    return list(processed.values())\n",
        "\n",
        "file = process_and_aggregate(file, reverse_synonyms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pohpQBm5EsSZ"
      },
      "outputs": [],
      "source": [
        "with open('proce.json', 'w') as f:\n",
        "    json.dump(file, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj6Bp8IaaSPJ"
      },
      "source": [
        "## Visualize KB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbga9MV_4Cl1"
      },
      "outputs": [],
      "source": [
        "def create_and_show_network(relations):\n",
        "    # Initialize a network graph\n",
        "    net = Network(directed=True, height=\"100vh\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
        "\n",
        "    # Add nodes and edges based on the relations data\n",
        "    for relation in relations:\n",
        "        net.add_node(relation['head'], label=relation['head'], title=relation['head'])\n",
        "        net.add_node(relation['tail'], label=relation['tail'], title=relation['tail'])\n",
        "        net.add_edge(relation['head'], relation['tail'], title=relation['type'])\n",
        "\n",
        "    # Define graph aesthetics and physics for better visualization\n",
        "    net.set_options(\"\"\"\n",
        "    {\n",
        "      \"nodes\": {\n",
        "        \"font\": {\n",
        "          \"size\": 12,\n",
        "          \"color\": \"#ffffff\"\n",
        "        }\n",
        "      },\n",
        "      \"edges\": {\n",
        "        \"arrows\": {\n",
        "          \"to\": {\n",
        "            \"enabled\": true,\n",
        "            \"scaleFactor\": 0.5\n",
        "          }\n",
        "        },\n",
        "        \"color\": {\n",
        "          \"inherit\": true\n",
        "        },\n",
        "        \"smooth\": {\n",
        "          \"type\": \"dynamic\"\n",
        "        }\n",
        "      },\n",
        "      \"physics\": {\n",
        "        \"forceAtlas2Based\": {\n",
        "          \"gravitationalConstant\": -26,\n",
        "          \"centralGravity\": 0.005,\n",
        "          \"springLength\": 230,\n",
        "          \"springConstant\": 0.18\n",
        "        },\n",
        "        \"maxVelocity\": 146,\n",
        "        \"minVelocity\": 0.1,\n",
        "        \"solver\": \"forceAtlas2Based\",\n",
        "        \"timestep\": 0.35,\n",
        "        \"stabilization\": { \"iterations\": 150 }\n",
        "      }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    # Save and display the network\n",
        "    net.show(\"relations_network.html\", notebook = False)\n",
        "    files.download('relations_network.html')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M13SN2Q_dJN6"
      },
      "outputs": [],
      "source": [
        "# Call the function to create and show the network\n",
        "create_and_show_network(kb.relations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kabfWxCTZnTf",
        "ekGnutl3ZzTz",
        "08RukxO23PsX",
        "0r6Ko6xjBwT0",
        "XMs4hbOS8E5e",
        "zj6Bp8IaaSPJ",
        "LxBSnpS1P2sX",
        "f3eGbhHtoeEA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
