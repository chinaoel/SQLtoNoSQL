{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\SQL\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\USER\\Desktop\\SQL\\myenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "\n",
    "# 配置BitsAndBytes的設定，用於模型的量化以提高效率。\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 啟用4位元量化\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算時使用的數據類型\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 量化類型\n",
    "    bnb_4bit_use_double_quant=True,  # 使用雙重量化\n",
    ")\n",
    "\n",
    "# 定義模型ID，用於從HuggingFace Hub加載模型。\n",
    "model_id = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "\n",
    "# 加載並配置模型，這裡使用了前面定義的量化配置。\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",  # 自動選擇運行設備\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "# 加載模型的分詞器。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 創建一個用於文本生成的pipeline。\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_4bit,\n",
    "    tokenizer=tokenizer,\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=3200,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 創建一個HuggingFacePipeline實例，用於後續的語言生成。\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''### Instruction:\n",
    "Given a prompt delimited by triple backticks, perform the following actions: \n",
    "- Convert the following natural language input into a Cypher query.\n",
    "- You should ignore the curly braces in the cypher query and only take question as input\n",
    "- You should ignore the input keys \n",
    "\n",
    "### Examples:\n",
    "\n",
    "\n",
    "### prompt: Retrieve all nodes connected to 'Harry Potter' with any relationship.\n",
    "\n",
    "### output: \n",
    "MATCH (harry:Object {{name: 'Harry Potter'}})--(connectedNodes) \n",
    "RETURN connectedNodes.\n",
    "\n",
    "### prompt: Get the names of all objects related to 'Hogwarts' with the relationship 'LOCATED_AT'.\n",
    "\n",
    "### output: \n",
    "MATCH (object)-[:LOCATED_AT]->(hogwarts:Object {{name: 'Hogwarts'}}) \n",
    "RETURN object.name.\n",
    "Ground truth information available: \n",
    "\n",
    "### prompt:```{question}```\n",
    "\n",
    "### output:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a prompt delimited by triple backticks, perform the following actions:\n",
    "- transform the prompt into a cypher query language, do not output other sentences, only cypher query language\n",
    "- output the cypher query language only, do not output other sentences \n",
    "- you only need to output the answer about prompt question, and eliminate the \\n in the end\n",
    "\n",
    "###prompt: ```{question}```\n",
    "###output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question):\n",
    "  prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "  response = llm_chain.run({\"question\":question})\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Given a prompt delimited by triple backticks, perform the following actions:\\n- transform the prompt into a cypher query language, do not output other sentences, only cypher query language\\n- output the cypher query language only, do not output other sentences \\n- you only need to output the answer about prompt question, and eliminate the \\n in the end\\n\\n###prompt: ```Who are the members of Ravenclaw?```\\n###output:\\n```MATCH (n:Character) WHERE n.house = 'Ravenclaw' RETURN n```\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = generate_response(\"Who are the members of Ravenclaw?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model = None\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM studio API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given a prompt delimited by triple backticks, perform the following actions:\n",
    "- transform the prompt into a cypher query language\n",
    "- output the cypher query language\n",
    "###prompt: ```{question}```\n",
    "###output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\" ```cpp\\nMATCH (h:House)-[:HAS_MEMBER]->(m:Member)\\nWHERE h.name = 'Ravenclaw'\\nRETURN m.name\\n```\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \n",
    "     '''Given a prompt delimited by triple backticks, perform the following actions:\n",
    "    - output the cypher query language only, do not output other sentences \n",
    "    - you only need to output the answer about prompt question, and eliminate the \\n in the end \n",
    "    '''},\n",
    "    {\"role\": \"user\", \"content\": \"You are given a knowledge graph. Note that the relations character is all lower and with no space in between and the node is Object with a name label. Use the provided knowledge graph to construct cypher query to answer the following question. Who are the members of Ravenclaw?\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
